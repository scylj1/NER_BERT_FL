{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CWhVdaQCeiq"
      },
      "source": [
        "Load training and development datasets. Print out some examples to show that the reading process is working properly. Also, print out some statistic information to analyse the features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEMLIV5jSBgG",
        "outputId": "c63055e5-df0b-4ce4-f7de-62900f6bc28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data information\n",
            "\n",
            "\n",
            "First 10 rows: \n",
            "\n",
            "       token label bio_only  upos\n",
            "0  @paulwalk     O        O  NOUN\n",
            "1         It     O        O  PRON\n",
            "2         's     O        O   AUX\n",
            "3        the     O        O   DET\n",
            "4       view     O        O  NOUN\n",
            "5       from     O        O   ADP\n",
            "6      where     O        O   ADV\n",
            "7          I     O        O  PRON\n",
            "8         'm     O        O     X\n",
            "9     living     O        O  NOUN\n",
            "\n",
            "Statistics: \n",
            "\n",
            "        token  label bio_only   upos\n",
            "count   62236  62241    62241  62241\n",
            "unique  14799     13        3     17\n",
            "top         .      O        O   NOUN\n",
            "freq     1920  59100    59100  12178\n",
            "\n",
            "Label statistics: \n",
            "\n",
            "O    59100\n",
            "B     1964\n",
            "I     1177\n",
            "Name: bio_only, dtype: int64\n",
            "\n",
            "Development data information\n",
            "\n",
            "\n",
            "First 10 rows: \n",
            "\n",
            "        token label bio_only   upos\n",
            "0  Stabilized     O        O  PROPN\n",
            "1    approach     O        O   NOUN\n",
            "2          or     O        O  CCONJ\n",
            "3         not     O        O   PART\n",
            "4           ?     O        O  PUNCT\n",
            "5        That     O        O   PRON\n",
            "6           Â´     O        O    SYM\n",
            "7           s     O        O   PART\n",
            "8      insane     O        O    ADJ\n",
            "9         and     O        O  CCONJ\n",
            "\n",
            "Statistics: \n",
            "\n",
            "        token  label bio_only   upos\n",
            "count   15382  15382    15382  15382\n",
            "unique   4015     13        3     17\n",
            "top         .      O        O   NOUN\n",
            "freq      882  14144    14144   2738\n",
            "\n",
            "Label statistics: \n",
            "\n",
            "O    14144\n",
            "B      826\n",
            "I      412\n",
            "Name: bio_only, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# data loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# read training data\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "\n",
        "# train data information\n",
        "print(\"Training data information\\n\")\n",
        "print(\"\\nFirst 10 rows: \\n\")\n",
        "print(train.head(n=10))\n",
        "print(\"\\nStatistics: \\n\")\n",
        "print(train.describe())\n",
        "print(\"\\nLabel statistics: \\n\")\n",
        "print(train['bio_only'].value_counts())\n",
        "\n",
        "# read development data\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "\n",
        "# development data information\n",
        "print(\"\\nDevelopment data information\\n\")\n",
        "print(\"\\nFirst 10 rows: \\n\")\n",
        "print(dev.head(n=10))\n",
        "print(\"\\nStatistics: \\n\")\n",
        "print(dev.describe())\n",
        "print(\"\\nLabel statistics: \\n\")\n",
        "print(dev['bio_only'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0BznZFjDRTd"
      },
      "source": [
        "The results show that the loading process is successful. From the statistics, we know that the numbers of different labels are very imballanced, so we have to use some algorithms to mitigate this effect. Also, we observe that some tokens appear very frequently, such as '.' and 'the'. These large number of punctuations and stopwords may contain noise when predicting name entities. Hence, it worth trying to remove these tokens and check the performance. \n",
        "\n",
        "The next step is data preprocessing. We will delete all the N/A values, because they are useless when training the models. In addition, we change the BIO labels and POS tags to integers for the future use in training process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJr41f1gfyyH",
        "outputId": "aace08c9-7a79-45be-ede6-cfd4d93b00f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        token       label bio_only   upos  bio_only_label  pos_indices\n",
            "0   @paulwalk           O        O   NOUN               2            0\n",
            "1          It           O        O   PRON               2            1\n",
            "2          's           O        O    AUX               2            2\n",
            "3         the           O        O    DET               2            3\n",
            "4        view           O        O   NOUN               2            0\n",
            "5        from           O        O    ADP               2            4\n",
            "6       where           O        O    ADV               2            5\n",
            "7           I           O        O   PRON               2            1\n",
            "8          'm           O        O      X               2            6\n",
            "9      living           O        O   NOUN               2            0\n",
            "10        for           O        O    ADP               2            4\n",
            "11        two           O        O    NUM               2            7\n",
            "12      weeks           O        O   NOUN               2            0\n",
            "13          .           O        O  PUNCT               2            8\n",
            "14     Empire  B-location        B  PROPN               0            9\n",
            "15      State  I-location        I  PROPN               1            9\n",
            "16   Building  I-location        I  PROPN               1            9\n",
            "17          =           O        O    SYM               2           10\n",
            "18        ESB  B-location        B  PROPN               0            9\n",
            "19          .           O        O  PUNCT               2            8\n"
          ]
        }
      ],
      "source": [
        "# data preprocessing\n",
        "# Drop empty rows between texts\n",
        "train = train.dropna()\n",
        "dev = dev.dropna()\n",
        "\n",
        "# Quantification of qualitative labels and features (BIO labels and POS tags)\n",
        "# Change BIO labels to integers\n",
        "def bio_index(bio):\n",
        "  if bio=='B':\n",
        "    ind = 0\n",
        "  elif bio=='I':\n",
        "    ind = 1\n",
        "  elif bio=='O':\n",
        "    ind = 2\n",
        "  return ind\n",
        "\n",
        "# Convert POS tags to integers\n",
        "# Get the UPOS tagset\n",
        "pos_vocab = train.upos.unique().tolist()\n",
        "# Convert POS-tags to integers\n",
        "def pos_index(pos):\n",
        "  ind = pos_vocab.index(pos)\n",
        "  return ind\n",
        "\n",
        "# Quantify BIO labels and POS tags\n",
        "def numeralization(txt):\n",
        "  txt_copy = txt.reset_index(drop=True) #  make a copy of original data frame\n",
        "\n",
        "  # BIO labels\n",
        "  bioints = [bio_index(b) for b in txt_copy['bio_only']]\n",
        "  txt_copy['bio_only_label'] = bioints\n",
        "\n",
        "  # POS tags\n",
        "  posinds = [pos_index(u) for u in txt_copy['upos']]\n",
        "  txt_copy['pos_indices'] = posinds\n",
        "\n",
        "  return txt_copy\n",
        "\n",
        "# Preprocess train and development data\n",
        "train_preprocess = numeralization(train)\n",
        "dev_preprocess = numeralization(dev)\n",
        "print(train_preprocess.head(n=20)) # check the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEaoS1Z5eqaf"
      },
      "source": [
        "We have aready got the POS as a feature, and we also want to explore more features from both word level and sentence level. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEAQiNTAf1LO",
        "outputId": "07b54e71-b4bd-4ec0-8f21-e0fd5533e722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        token       label bio_only   upos  bio_only_label  pos_indices  x_pre  \\\n",
            "0   @paulwalk           O        O   NOUN               2            0     -1   \n",
            "1          It           O        O   PRON               2            1      0   \n",
            "2          's           O        O    AUX               2            2      1   \n",
            "3         the           O        O    DET               2            3      2   \n",
            "4        view           O        O   NOUN               2            0      3   \n",
            "5        from           O        O    ADP               2            4      0   \n",
            "6       where           O        O    ADV               2            5      4   \n",
            "7           I           O        O   PRON               2            1      5   \n",
            "8          'm           O        O      X               2            6      1   \n",
            "9      living           O        O   NOUN               2            0      6   \n",
            "10        for           O        O    ADP               2            4      0   \n",
            "11        two           O        O    NUM               2            7      4   \n",
            "12      weeks           O        O   NOUN               2            0      7   \n",
            "13          .           O        O  PUNCT               2            8      0   \n",
            "14     Empire  B-location        B  PROPN               0            9      8   \n",
            "15      State  I-location        I  PROPN               1            9      9   \n",
            "16   Building  I-location        I  PROPN               1            9      9   \n",
            "17          =           O        O    SYM               2           10      9   \n",
            "18        ESB  B-location        B  PROPN               0            9     10   \n",
            "19          .           O        O  PUNCT               2            8      9   \n",
            "\n",
            "    x_aft  is_propn  is_noun  has_capital  is_title  is_digit  word_length  \n",
            "0       1     False     True        False     False     False            9  \n",
            "1       2     False    False         True      True     False            2  \n",
            "2       3     False    False        False     False     False            2  \n",
            "3       0     False    False        False     False     False            3  \n",
            "4       4     False     True        False     False     False            4  \n",
            "5       5     False    False        False     False     False            4  \n",
            "6       1     False    False        False     False     False            5  \n",
            "7       6     False    False         True      True     False            1  \n",
            "8       0     False    False        False     False     False            2  \n",
            "9       4     False     True        False     False     False            6  \n",
            "10      7     False    False        False     False     False            3  \n",
            "11      0     False    False        False     False     False            3  \n",
            "12      8     False     True        False     False     False            5  \n",
            "13      9     False    False         True     False     False            1  \n",
            "14      9      True    False         True      True     False            6  \n",
            "15      9      True    False         True      True     False            5  \n",
            "16     10      True    False         True      True     False            8  \n",
            "17      9     False    False         True     False     False            1  \n",
            "18      8      True    False         True     False     False            3  \n",
            "19      5     False    False         True     False     False            1  \n"
          ]
        }
      ],
      "source": [
        "# feature extraction\n",
        "# feature 1: part-of-speech index\n",
        "\n",
        "# feature 2: the POS index of the previous token, \n",
        "# because we want to capture some context information before the token\n",
        "def pre_index(pos):\n",
        "  return np.insert(pos[0:-1], 0 , [-1]) # The first token has no previous token, so it is set to -1\n",
        "\n",
        "# feature 3: the POS index of the next token, \n",
        "# because we want to capture some context information after the token\n",
        "def aft_index(pos):\n",
        "  return np.append(pos[1 : ], -1) # The last token has no next token, so it is set to -1\n",
        "\n",
        "# feature 4: is this token a proper noun?\n",
        "# A proper noun is more likely to be a name entity than other POS \n",
        "def is_propn(pos):\n",
        "  resp = False\n",
        "  if pos=='PROPN':\n",
        "    resp = True\n",
        "  return resp\n",
        "\n",
        "# feature 5: is this token a noun?\n",
        "# A noun is also more likely to be a name entity than other POS\n",
        "def is_noun(pos):\n",
        "  resp = False\n",
        "  if pos=='NOUN':\n",
        "    resp = True\n",
        "  return resp\n",
        "\n",
        "# feature 6: Does the token contain capital letters?\n",
        "# If a token contains captital letter, it is likely to be a name entity\n",
        "def has_capital(tok):\n",
        "  return not (tok.islower())\n",
        "\n",
        "# feature 7: is the token a common title?\n",
        "# A title is likely to be a name entity\n",
        "def is_title(tok):\n",
        "  return tok.istitle()\n",
        "\n",
        "# feature 8: Does the token consist of digits?\n",
        "# Some name entities consist of digits. \n",
        "def is_digit(tok):\n",
        "  return tok.isdigit()\n",
        "\n",
        "\n",
        "# feature 9: the length of the token\n",
        "# Some name entities have long word length\n",
        "def word_length(tok):\n",
        "  return len(tok)\n",
        "\n",
        "# extract features using the above functions\n",
        "def extract_features(txt):\n",
        "  txt_copy = txt.reset_index(drop=True)\n",
        "\n",
        "  posinds = [pos_index(u) for u in txt_copy['upos']] # POS tag index\n",
        "\n",
        "  x_pre = pre_index(posinds)\n",
        "  txt_copy['x_pre'] = x_pre\n",
        "\n",
        "  x_aft = aft_index(posinds)\n",
        "  txt_copy['x_aft'] = x_aft\n",
        "\n",
        "  isprop = [is_propn(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_propn'] = isprop\n",
        "\n",
        "  isnoun = [is_noun(u) for u in txt_copy['upos']]\n",
        "  txt_copy['is_noun'] = isnoun\n",
        "\n",
        "  capital = [has_capital(t) for t in txt_copy['token']]\n",
        "  txt_copy['has_capital'] = capital\n",
        "\n",
        "  title = [is_title(t) for t in txt_copy['token']]\n",
        "  txt_copy['is_title'] = title\n",
        "\n",
        "  digit = [is_digit(t) for t in txt_copy['token']]\n",
        "  txt_copy['is_digit'] = digit\n",
        "\n",
        "  wlength = [word_length(t) for t in txt_copy['token']]\n",
        "  txt_copy['word_length'] = wlength\n",
        "\n",
        "  return txt_copy\n",
        "\n",
        "train_with_feature = extract_features(train_preprocess) # extract features from preprocessed training set\n",
        "dev_with_feature = extract_features(dev_preprocess) # # extract features from preprocessed development set\n",
        "print(train_with_feature.head(n=20)) # check the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY1JOF1EUuIC"
      },
      "source": [
        "The next step is to train the model. Because the training classes are very imbalanced, we aim to use the easy ensemble method to tackle the problem. The majority class set ('O') is divided into some subsets, and we make the number of samples in each subset is similar to the number in minority classes ('B' + 'I', about 3,000 samples). Then, each majority sample subset is combined with the minority samples to obtain a down-sampled set, and we use a base classifier for training. Finally, bagging all the base classifiers to obtain the final model.\n",
        "\n",
        "For the base classifier, we use a voting model to combine the advantages of different classifiers. It will get a better performance comparing with using a single model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcX4sGJSASmg",
        "outputId": "b6725e84-6256-4f05-83ca-836db7f7e125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Subset 0 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3363\n",
            "top          .\n",
            "freq       113\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 1 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3386\n",
            "top          .\n",
            "freq       112\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 2 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3329\n",
            "top          .\n",
            "freq       119\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 3 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3400\n",
            "top          .\n",
            "freq        98\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 4 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3350\n",
            "top          .\n",
            "freq       116\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 5 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3380\n",
            "top          .\n",
            "freq       117\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 6 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3361\n",
            "top          .\n",
            "freq       135\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 7 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3356\n",
            "top          .\n",
            "freq       112\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 8 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3344\n",
            "top          .\n",
            "freq       111\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 9 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3342\n",
            "top          .\n",
            "freq        93\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 10 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3385\n",
            "top          .\n",
            "freq       114\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 11 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3375\n",
            "top          .\n",
            "freq       125\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 12 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3361\n",
            "top          .\n",
            "freq       127\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 13 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3410\n",
            "top          .\n",
            "freq       113\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 14 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3499\n",
            "top          .\n",
            "freq       103\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 15 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3576\n",
            "top          .\n",
            "freq       101\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 16 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3500\n",
            "top          :\n",
            "freq        96\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 17 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3619\n",
            "top          :\n",
            "freq        88\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 18 token statistic:\n",
            "\n",
            "count     6141\n",
            "unique    3529\n",
            "top          :\n",
            "freq        98\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 19 token statistic:\n",
            "\n",
            "count     5236\n",
            "unique    3174\n",
            "top          .\n",
            "freq        75\n",
            "Name: token, dtype: object\n",
            "training finished\n"
          ]
        }
      ],
      "source": [
        "# model training\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "'''\n",
        "This function takes the whole training set and an index as inputs.\n",
        "It returns a down-sampled dataset, containing all data labelled with 'B' or 'I', and 3,000 data labelled with 'O'. \n",
        "The input index determines which 3,000 'O' are chosen. For example, if index is 0, the first 3,000 data labelled with 'O' will be selected. \n",
        "'''\n",
        "def train_split(txt, index):\n",
        "  txt_copy = txt.reset_index(drop=True)\n",
        "\n",
        "  # split into B&I versus O subsets\n",
        "  is_inside = txt_copy['bio_only']!='O' # the indexes of data labelled with 'B' & 'I'\n",
        "  is_outside = txt_copy['bio_only']=='O' # the indexes of data labelled with 'O'\n",
        "  bi = txt_copy[is_inside] # All 'B' & 'I'\n",
        "  outside = txt_copy[is_outside] # All 'O'\n",
        "\n",
        "  # Choose 3,000 'O'. If the index is out of bound, select all the data after the input index\n",
        "  if (index + 3000) < len(outside['bio_only']):\n",
        "    outside = outside[index : index + 3000]  # approx the sum of B and I labels in train\n",
        "  else:\n",
        "    outside = outside[index :  ]\n",
        "\n",
        "  # recombine\n",
        "  down_sample = pd.concat([bi, outside])\n",
        "\n",
        "  # check the results\n",
        "  print('\\nSubset %d token statistic:\\n' % int(index/3000))\n",
        "  print(down_sample['token'].describe())\n",
        "  return down_sample\n",
        "\n",
        "'''\n",
        "This function takes the training set and number of base classifiers as inputs.\n",
        "It will return a list of base models. Each base model will train on different down-sampled dataset. \n",
        "'''\n",
        "def train_model(train_set, num_clf):\n",
        "  models = []\n",
        "  # Train a lists of models, the number of models is equal to 'num_clf'\n",
        "  for i in range (0 , num_clf): \n",
        "    # Obtain a down-sampled set \n",
        "    train_subset = train_split(train_set, i*3000)\n",
        "\n",
        "    # Extract the features used for training\n",
        "    x_train = train_subset.drop(['token', 'label', 'bio_only', 'upos' , 'bio_only_label' ], axis=1)\n",
        "    # Extract the labels\n",
        "    y_train = train_subset['bio_only_label']\n",
        "\n",
        "    # Define some basic models to consist a vote model\n",
        "    ''' \n",
        "    Different classifiers have different advantages and shortages on this tasks. The reason to use a vote model is to combine the advantages\n",
        "    from different classifiers. Hence, we select some commonly used and well-performed classifiers to consist the vote model. \n",
        "    Continuing problems would be investigating which basic models should be use, the number of models and the hyper-parameters in each model.\n",
        "\n",
        "    '''\n",
        "    model_gbc = GradientBoostingClassifier()\n",
        "    model_lgbmc = LGBMClassifier()\n",
        "    model_xgbc = XGBClassifier()\n",
        "    model_rf = RandomForestClassifier()\n",
        "    model_lr = LogisticRegression(multi_class='multinomial')\n",
        "    model_BagC = BaggingClassifier()\n",
        "\n",
        "    # Define the vote model\n",
        "    estimators = [('randomforest', model_rf),('logistic', model_lr),('bagging', model_BagC),('gbc',model_gbc), ('lgbmc',model_lgbmc), ('xgbc', model_xgbc) ]\n",
        "    model = VotingClassifier(estimators=estimators, voting='soft', weights=[1, 5, 1, 1, 1, 1], n_jobs=-1) \n",
        "    # The weights are set based on simply trying some different numbers. A continuing problem is to investigate all the hyper-parameters. \n",
        "\n",
        "    # train the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # append to the model list\n",
        "    models.append(model)\n",
        "\n",
        "  return models\n",
        "\n",
        "# train models\n",
        "models = train_model(train_with_feature, 20) # there are approx 60,000 data labelled 'O', so the input is set to 60000/3000=20\n",
        "print('training finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLxPT_vTyTcN"
      },
      "source": [
        "The statistic information from each subset is different. We can concluded that different base models trained on different down-sampled sets, which means the code works as intended.\n",
        "\n",
        "When predicting results, each base models will give probabilities on 3 labels. For each input, we calculate the sum of probabilities from all base models on each label, and select the label with highest likelihood as the final result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JgCPR_9y-zQr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function takes the model lists and development dataset as input.\n",
        "It returns the predict labels as results\n",
        "'''\n",
        "def predict_ensemble(model_list, x_development):\n",
        "  prob = []\n",
        "  # Calculate the sum of probabilities\n",
        "  for m in model_list:\n",
        "    if m == model_list[0]:\n",
        "      prob = m.predict_proba(x_development)\n",
        "    else:\n",
        "      prob = prob + m.predict_proba(x_development)\n",
        "\n",
        "  results = []\n",
        "  # Select the label with highest probability as result\n",
        "  for line in prob:\n",
        "    res = np.argmax(line)\n",
        "    results.append(res)\n",
        " \n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Xg0TFe4nzC"
      },
      "source": [
        "We can use this model to validate on the development set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiC4IalY4CET",
        "outputId": "ed6cf48b-e6ee-4b85-eb31-6b7451c5a41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted label, Count of labels\n",
            "[[    0  1088]\n",
            " [    1   280]\n",
            " [    2 14014]]\n"
          ]
        }
      ],
      "source": [
        "# Extract the features used for predicting\n",
        "X_dev = dev_with_feature.drop(['token', 'label', 'bio_only', 'upos' , 'bio_only_label'], axis=1)\n",
        "X_dev.head()\n",
        "\n",
        "# Generate predictions\n",
        "preds = predict_ensemble(models, X_dev)\n",
        "\n",
        "# Check if our classifier has only predicted outside=2 for all tokens in the dev file\n",
        "(unique, counts) = np.unique(preds, return_counts=True)\n",
        "print('Predicted label, Count of labels')\n",
        "print(np.asarray((unique, counts)).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vyDRLdh44uL"
      },
      "source": [
        "The result looks plausible, and we can further evaluate on precision, recall and F1 scores. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzG1x7IOuL25",
        "outputId": "2bf72058-c22d-4b1b-cf3e-01b6851aff37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New evaluation:\n",
            "Sum of TP and FP = 1088\n",
            "Sum of TP and FN = 826\n",
            "True positives = 422, False positives = 666, False negatives = 404\n",
            "Precision = 0.388, Recall = 0.511, F1 = 0.441\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "# The code is copy from 'Task 2' notebook\n",
        "def wnut_evaluate(txt):\n",
        "  '''entity evaluation: we evaluate by whole named entities'''\n",
        "  npred = 0; ngold = 0; tp = 0\n",
        "  nrows = len(txt)\n",
        "  for i in txt.index:\n",
        "    if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
        "      npred += 1\n",
        "      ngold += 1\n",
        "      for predfindbo in range((i+1),nrows):\n",
        "        if txt['prediction'][predfindbo]=='O' or txt['prediction'][predfindbo]=='B':\n",
        "          break  # find index of first O (end of entity) or B (new entity)\n",
        "      for goldfindbo in range((i+1),nrows):\n",
        "        if txt['bio_only'][goldfindbo]=='O' or txt['bio_only'][goldfindbo]=='B':\n",
        "          break  # find index of first O (end of entity) or B (new entity)\n",
        "      if predfindbo==goldfindbo:  # only count a true positive if the whole entity phrase matches\n",
        "        tp += 1\n",
        "    elif txt['prediction'][i]=='B':\n",
        "      npred += 1\n",
        "    elif txt['bio_only'][i]=='B':\n",
        "      ngold += 1\n",
        "  \n",
        "  fp = npred - tp  # n false predictions\n",
        "  fn = ngold - tp  # n missing gold entities\n",
        "  prec = tp / (tp+fp)\n",
        "  rec = tp / (tp+fn)\n",
        "  f1 = (2*(prec*rec)) / (prec+rec)\n",
        "  print('Sum of TP and FP = %i' % (tp+fp))\n",
        "  print('Sum of TP and FN = %i' % (tp+fn))\n",
        "  print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
        "  print('Precision = %.3f, Recall = %.3f, F1 = %.3f' % (prec, rec, f1))\n",
        "\n",
        "# reverse BIO labels from integers to 'B' 'I' and 'O'\n",
        "def reverse_bio(ind):\n",
        "  bio = ' '\n",
        "  if ind==0:\n",
        "    bio = 'B'\n",
        "  elif ind==1:\n",
        "    bio = 'I'\n",
        "  elif ind==2:\n",
        "    bio = 'O'\n",
        "  return bio\n",
        "\n",
        "# Convert BIO labels to original form\n",
        "bio_preds = [reverse_bio(p) for p in preds]\n",
        "dev_with_feature['prediction'] = bio_preds\n",
        "\n",
        "# Evaluate on development set\n",
        "print('New evaluation:')\n",
        "wnut_evaluate(dev_with_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYAET3Ry6u_D"
      },
      "source": [
        "The result is not too bad. It shows that the easy ensemble method can improve performance on imbalanced dataset. \n",
        "\n",
        "We can try to improve the performance by removing the punctuations and stopwords, because they may introduce noise during training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49hWoF1Z79Ye",
        "outputId": "4167c6da-962f-4845-820d-a0a5287a8de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O    51569\n",
            "B     1964\n",
            "I     1134\n",
            "Name: bio_only, dtype: int64\n",
            "count     54667\n",
            "unique    14657\n",
            "top         the\n",
            "freq       1105\n",
            "Name: token, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Remove punctuation\n",
        "not_punct = train_with_feature['upos']!='PUNCT'\n",
        "train_no_punc = train_with_feature[not_punct]\n",
        "print(train_no_punc['bio_only'].value_counts())\n",
        "print(train_no_punc['token'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubNYNDPB8YPB"
      },
      "source": [
        "About 8,000 samples labelled with 'O' are removed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yR1xVxVKpSN",
        "outputId": "f0aaf7ef-cdc5-4465-ced2-18e6dcb2a205"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O    36010\n",
            "B     1948\n",
            "I     1096\n",
            "Name: bio_only, dtype: int64\n",
            "count     39054\n",
            "unique    14512\n",
            "top           I\n",
            "freq        870\n",
            "Name: token, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "not_stop = [n not in stop_words for n in train_no_punc['token']]\n",
        "train_no_punc_stop = train_no_punc[not_stop]\n",
        "print(train_no_punc_stop['bio_only'].value_counts())\n",
        "print(train_no_punc_stop['token'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvTyidOi9icL"
      },
      "source": [
        "About 15,000 samples labelled with 'O' are removed. Although a few 'B' and 'I' are removed, they will not cause significant affect. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kdIzH4WepLv",
        "outputId": "1844fb19-f12a-4bbc-bd52-4b6e4faf790f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Subset 0 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3830\n",
            "top          I\n",
            "freq       102\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 1 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3793\n",
            "top          I\n",
            "freq        99\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 2 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3798\n",
            "top          I\n",
            "freq        86\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 3 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3823\n",
            "top          I\n",
            "freq        93\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 4 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3785\n",
            "top          I\n",
            "freq        98\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 5 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3823\n",
            "top          I\n",
            "freq        87\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 6 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3823\n",
            "top          I\n",
            "freq        83\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 7 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3836\n",
            "top          I\n",
            "freq        80\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 8 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3947\n",
            "top          I\n",
            "freq        59\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 9 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3975\n",
            "top         RT\n",
            "freq        95\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 10 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    4007\n",
            "top         RT\n",
            "freq        93\n",
            "Name: token, dtype: object\n",
            "\n",
            "Subset 11 token statistic:\n",
            "\n",
            "count     6044\n",
            "unique    3959\n",
            "top         RT\n",
            "freq        79\n",
            "Name: token, dtype: object\n",
            "New evaluation:\n",
            "Sum of TP and FP = 948\n",
            "Sum of TP and FN = 826\n",
            "True positives = 401, False positives = 547, False negatives = 425\n",
            "Precision = 0.423, Recall = 0.485, F1 = 0.452\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the results\n",
        "# train the model\n",
        "new_models = train_model(train_no_punc_stop, 12) # The second parameter is set to 36000/3000 = 12\n",
        "# predict results\n",
        "preds = predict_ensemble(new_models, X_dev)\n",
        "# convert the results from integer to 'B' 'I' 'O'\n",
        "bio_preds = [reverse_bio(p) for p in preds]\n",
        "dev_with_feature['prediction'] = bio_preds\n",
        "# evaluate\n",
        "print('New evaluation:')\n",
        "wnut_evaluate(dev_with_feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpeRNKkvAdR6"
      },
      "source": [
        "The F1 score increaces by about 0.01. From the results, we find that the model predict more 'B' and less 'I' than expected. Some tokens labelled with 'I' are predicted as 'B' by the model. A continuing problem is to improve on this.\n",
        "\n",
        "We can finally apply it on the test set and check the performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4kVC2FyA38B",
        "outputId": "5b18046f-f783-4a3a-ff9f-07a9f9c1edd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        token   upos\n",
            "count   23323  23323\n",
            "unique   6329     17\n",
            "top         .   NOUN\n",
            "freq      962   4442\n",
            "        token   upos prediction\n",
            "count   23323  23323      23323\n",
            "unique   6329     17          3\n",
            "top         .   NOUN          O\n",
            "freq      962   4442      21370\n",
            "O    21370\n",
            "B     1580\n",
            "I      373\n",
            "Name: prediction, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Generate test results\n",
        "# load data\n",
        "wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'\n",
        "testset = pd.read_table(wnuttest, header=None, names=['token', 'upos']).dropna()\n",
        "print(testset.describe())\n",
        "\n",
        "# preprocess\n",
        "test_preprocess = testset.reset_index(drop=True)\n",
        "posinds = [pos_index(u) for u in test_preprocess['upos']]\n",
        "test_preprocess['pos_indices'] = posinds\n",
        "\n",
        "# Extract the features used for predicting\n",
        "test_with_feature = extract_features(test_preprocess)\n",
        "X_test = test_with_feature.drop(['token', 'upos'], axis=1)\n",
        "\n",
        "# Generating predictions\n",
        "preds = predict_ensemble(new_models, X_test)\n",
        "\n",
        "# Convert results\n",
        "bio_preds = [reverse_bio(p) for p in preds]\n",
        "testset['prediction'] = bio_preds\n",
        "\n",
        "# Save to txt file\n",
        "print(testset.describe())\n",
        "print(testset['prediction'].value_counts())\n",
        "testset.to_csv('test2.txt', sep='\\t', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
